import os
import time
from openai import OpenAI
import anthropic
import google.generativeai as genai
import gradio as gr
from dotenv import load_dotenv

# Carica variabili d'ambiente
load_dotenv()

openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')

# Debug sulle chiavi
if openai_api_key:
    print(f"OpenAI API Key exists and starts with {openai_api_key[:8]}")
else:
    print("OpenAI API Key not configured")

if anthropic_api_key:
    print(f"Anthropic API Key exists and starts with {anthropic_api_key[:7]}")
else:
    print("Anthropic API Key not configured")

if google_api_key:
    print(f"Google API Key exists and starts with {google_api_key[:8]}")
else:
    print("Google API Key not configured")

# Inizializzazione API
openai = OpenAI(api_key=openai_api_key)
claude = anthropic.Anthropic(api_key=anthropic_api_key)
genai.configure(api_key=google_api_key)

system_message = "Sei un assistente virtuale"

def stream_gpt(prompt):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]
    stream = openai.chat.completions.create(
        model='gpt-4o-mini',
        messages=messages,
        stream=True
    )
    result = ""
    for chunk in stream:
        result += chunk.choices[0].delta.content or ""
        yield result

def stream_claude(prompt):
    result = claude.messages.stream(
        model="claude-3-haiku-20240307",
        max_tokens=1000,
        temperature=0.7,
        system=system_message,
        messages=[
            {"role": "user", "content": prompt},
        ],
    )
    response = ""
    with result as stream:
        for text in stream.text_stream:
            response += text or ""
            yield response

def stream_gemini(prompt):
    # CORREZIONE: Passa il nome del modello come argomento diretto, non con "model="
    model = genai.GenerativeModel("gemini-2.5-flash") 
    
    stream = model.generate_content(prompt, stream=True)
    
    result = ""
    for chunk in stream:
        if chunk.text:
            result += chunk.text
            yield result

def stream_model(prompt, model):
    if model == "GPT":
        result = stream_gpt(prompt)
    elif model == "Claude":
        result = stream_claude(prompt)
    elif model == "Gemini":
        result = stream_gemini(prompt)
    else:
        raise ValueError("Unknown model")
    yield from result

# Interfaccia Gradio
view = gr.Interface(
    fn=stream_model,
    inputs=[
        gr.Textbox(label="Tu mensaje:"),
        gr.Dropdown(["GPT", "Claude", "Gemini"], label="Selecciona un modello:", value="GPT")
    ],
    outputs=[gr.Markdown(label="Respuesta:")],
    flagging_mode="never"
)

view.launch()
